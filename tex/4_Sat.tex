%\section{}
The problem of determining whether a formula in classical propositional logic is
satisfiable has the historical honor of being the first problem ever shown to be
NP-Complete~\cite{Cook}. Great efforts have been directed in improving the
efficiency of solvers for this problem, known as \emph{Boolean Satisfiability
Solvers}, or just \emph{SAT solvers}. Despite the worst-case deterministic
exponential run time of all the algorithms known, satisfiability solvers are
increasingly leaving their mark as a general purpose tool in the most diverse
areas~\cite{satchapter}. In essence, SAT solvers provide a generic combinatorial
reasoning and search platform for such problems.  Beyond that, the source code
of many implementations of such solvers is freely available and can be used as a
basis for the development of decision procedures for more expressive
logics~\cite{giunchiglia2002sat}.

In the context of SAT solvers for propositional provers, the underlying
representation formalism is propositional logic~\cite{satchapter}. We are
interested in formulae in \emph{Conjunctive Normal Form} (CNF): a formula
$\formula$ is in CNF if it is a conjunction of clauses. For example, $\formula =
(p \lor \neg q) \land (\neg p \lor r \lor s) \land (q \lor r)$ is a CNF formula
with four variables and three clauses. A clause with only one literal is
referred to as a \emph{unit clause}, and a clause with two literals, as a
\emph{binary clause}.

%% satisfying assignment %%
A propositional formula $\formula$ takes a value in the set $\{\false, \true\}$. In
algorithms for SAT, variables can be \emph{assigned} a logical value in this same
set and, alternatively, variables may also be \emph{unassigned}. A \emph{truth
assignment} (or just an assignment) to the set of variables $\Prop$, is the valuation
function $\pi$ as defined in Definition~\ref{def:semantics}. As in propositional
logic we have a unit set as the set of possible worlds $\St$, we can omit this
set from the function signature and just write $\pi : \Prop \longrightarrow
\{\false,\true\}$, for simplicity. A \emph{satisfying assignment} for $\formula$
is an assignment $\pi$ such that $\formula$ evaluates to $\true$ under $\pi$.  A
\emph{partial assignment} for a formula $\formula$ is a truth assignment to a
subset of the variables in $\formula$. For a partial assignment $\passignment$
for a CNF formula $\formula$, $\formula|_\passignment$ denotes the simplified
formula obtained by replacing the variables appearing in $\passignment$ with
their specified values, removing all clauses with at least one $\true$ literal,
and deleting all occurrences of $\false$ literals from the remaining
clauses~\cite{satchapter}.   

Therefore, the \emph{Boolean Satisfiability Problem} (SAT) can be expressed as:
Given a CNF formula $\formula$, does $\formula$ have a satisfying assignment? If
this is the case, $\formula$ is said to be \emph{satisfiable}, otherwise,
$\formula$ is \emph{unsatisfiable}.  One can be interested not only in the
answer of this decision problem, but also in finding the actual assignment that
satisfies the formula, when it exists. All practical SAT solvers do produce such
an assignment~\cite{cormen}. 

\section{The DPLL Procedure}%
\label{sec:dpll}

A \emph{complete} solution method for the SAT problem is one that, given the
input formula $\formula$, either produces a satisfying assignment for $\formula$
or proves that it is unsatisfiable~\cite{satchapter}. One of the most surprising
aspects of the relatively recent practical progress of SAT solvers is that the
best complete methods remain variants of a process introduced in the early
1960â€™s: the Davis-Putnam-Logemann-Loveland, or DPLL,
procedure~\cite{DavisLongemannLoveland:1962}, which describes a backtracking
algorithm to the search problem of finding a satisfying assignment for a formula in
the space of partial assignments. A key feature of DPLL is the efficient pruning of
the search space based on falsified clauses. Since its introduction, the main
improvements to DPLL have been smart branch selection heuristics, extensions
like clause learning and randomized restarts, and well-crafted data structures
such as lazy implementations and watched literals for fast unit
propagation~\cite{satchapter}.

Algorithm~\ref{alg:dpll}, DPLL-recursive$(\formula, \passignment)$ sketches the
basic DPLL procedure on CNF formulae~\cite{DavisLongemannLoveland:1962}, where
$\passignment$ corresponds to a partial assignment of the CNF formula
$\formula$. The main idea is to repeatedly select an unassigned literal $l$ in
the input formula and recursively search for a satisfying assignment for
$\formula|_l$ and $\formula|_{\neg l}$. The step where such an $l$ is chosen is
called a \emph{branching step} and $l$ is referred as a \emph{decision
variable}. Setting the decision variable to $\true$ or $\false$ when making a
recursive call is referred to as a \emph{decision}, which is associated with a
\emph{decision level} and it is equal to the recursion depth at that stage of
the procedure. The end of each recursive call, which takes $\formula$ back to
fewer assigned literals, is called the \emph{backtracking step}.

\begin{algorithm}[htp]%
    \SetAlgoLined\DontPrintSemicolon%
    \SetKwFunction{proc}{UnitPropagate}
    $(\formula, \passignment) \leftarrow$ \proc{$\formula,\passignment$}

    \If{$\formula$ contains the empty clause}
    {\Return~UNSAT}
    \If{$\formula$ has no clauses left}
    {Output $\passignment$\\
    \Return{SAT}
    }
    $l \leftarrow$ a literal not assigned by $\passignment$

    \If{DPLL-recursive$(\formula|_l, \passignment \cup \{l\}) = $ SAT} 
    {\Return{SAT}}
    \Return{DPLL-recursive$(\formula|_{\neg l}, \passignment \cup \{\neg l\})$}

    \vspace{2mm}
    \setcounter{AlgoLine}{0}
    \SetKwProg{myproc}{sub}{}{}
    \myproc{\proc{$\formula, \passignment$}}{%
    \While{$\formula$ contains no empty clause but has a unit clause $\clause$}
    {%
        $l \leftarrow$ the literal in $\clause$ not assigned by $\passignment$\\
        $\formula \leftarrow \formula|_l$\\
        $\passignment \leftarrow \passignment \cup \{l\}$
    }
    \Return{$(\formula, \passignment)$}
    }
    \caption{DPLL-recursive$(\formula, \passignment$)}%
    \label{alg:dpll}
\end{algorithm} 

A partial assignment $\passignment$ is maintained during the search and it is
output if the formula turns out to be satisfiable.  To increase efficiency, a
key procedure in SAT solvers is the \emph{unit propagation
procesure}~\cite{cdclchapter}, where unit clauses are immediately set to $\true$
as outlined in Algorithm~\ref{alg:dpll}. In most implementations of DPLL,
logical inferences can be derived with unit propagation.  Thus, this procedure
is used for identifying variables which must be assigned a specific value. If
$\formula |_\passignment$ contains the empty clause, a \emph{conflict} condition
is declared, the corresponding clause of $\formula$ from which it came is said
to be \emph{violated} by $\passignment$, and the algorithm backtracks. 

The literals whose negation do not appear in the formula, called
\emph{pure literals}, are also set to $\true$ as a preprocessing step and, in
some implementations, during the simplification process after every branching. 
We mentioned pure literal elimination in Chapter~\ref{3_Resolution} as the
purity principle proposed by Robinson.

Variants of the DPLL algorithm form the most widely used family of complete
algorithms for the SAT problem. They are frequently implemented in an iterative
manner, instead of using recursion, resulting in significantly reduced memory
usage. The efficiency of state-of-the-art SAT solvers relies heavily on various
features that have been developed, analysed and tested over the last two
decades. These include fast unit propagation using watched literals,
deterministic and randomized restart strategies, effective clause deletion
mechanisms, smart static and dynamic branching heuristics and learning
mechanisms. We will discuss learning mechanisms in the next section and refer
the reader to~\cite{satchapter} for more details about other search strategies.

\section{Conflict-Driven Clause Learning}%
\label{sec:cdcl}

One of the main reasons for the widespread use of SAT in many applications is
that solvers based on clause learning are effective in
practice~\cite{satchapter}. The main idea behind \emph{Conflict-Driven Clause
Learning} (CDCL) is to cache ``causes of conflict'' as learned clauses, and
utilize this information to prune the search in a different part of the search
space encountered later. Since their inception in the mid-90s, CDCL SAT solvers
have been applied, in many cases with remarkable success, to a number of
practical applications~\cite{cdclchapter}. The organization of CDCL SAT solvers
is primarily inspired by the DPLL procedure. The notation used in this section
is based on the one in~\cite{cdclchapter}.

In CDCL SAT solvers, each variable $p$ is characterized by a number of
properties. Mainly, we need to maintain its \emph{value}, its \emph{antecedent}
and its decision level.  A variable's value is denoted by $\nu(p)$, and defined
in the set $\{\false, \true, u\}$, where $\nu(p) = u$ means that $p$ is still
unassigned. 

A variable $p$ that is assigned a value as the result of unit propagation is
said to be \emph{implied}.  The unit clause $\clause$ used for implying this
value is said to be the antecedent of $p$, and it is denoted by $\alpha(p) =
\clause$. For decision variables or unassigned variables, the antecedent is
$nil$. Ergo, antecedents are only defined for variables whose value is implied
by other assignments. 

The decision level of a variable $p$, written $\delta(p)$, denotes the depth of
the decision tree at which the variables is assigned a value in $\{\false,
\true\}$ or $\delta(p) = -1$ if the value of $p$ is still unassigned, therefore,
$\delta(p) \in \{-1, 0, 1, \ldots, |\Prop_\formula|\}$, where $\Prop_\formula$
denotes the set of all variables that appear on the initial formula $\formula$.
The decision level associated with variables used for branching steps is
specified by the search process, and denotes the current depth of the
\emph{decision stack}.  Hence, a variable $p$ associated with a decision
assignment is characterized by having $\alpha(p) = nil$ and $\delta(p) > 0$.
More formally, the decision level of $p$ with antecedent $\clause$ is given by: 
\begin{equation}
    \delta(p) = \max(\{0\} \cup \{\delta(p') \mid p' \in \clause \land p' \neq p\})
\end{equation}
i.e.\ the decision of an implied literal is either the highest decision level of
the implied literals in a unit clause, or it is 0 in case the clause is unit.
The notation $p = v @ d$ is used to denote that $\nu(p) = v$ and $\delta(p) =
d$. Moreover, the decision level of a literal is defined as the decision level
of its variable, that is, $\delta(l) = \delta(p)$ if $l = p$ or $l = \neg p$.

\begin{example}
    Consider the formula 
    \begin{align*}
        \formula & = \clause_1 \land \clause_2 \land \clause_3\\
                 & = (p \lor \neg s) \land (p \lor r) \land (\neg r \lor q \lor s)
    \end{align*}
    Assume that the decision assignment is $s = \false @ 1$. Unit propagation
    yields no additional implied assignments. Assume that the second decision is
    $p = \false @ 2$. Unit propagation yields the implied assignments $r = \true
    @ 2$ and $q = \true @ 2$. Therefore, $\pi = \{(s, \false), (p, \false), (r,
    \true), (q, \true)\}$ is a satisfying assignment for $\formula$, since $\pi$
    makes $\formula$ true. Moreover, $\alpha(r) = \clause_2$ and $\alpha(q) =
    \clause_3$.
\end{example}

During the execution of a DPLL based SAT solver, assigned variables, as well as
their antecedents, define a directed acyclic graph $I = (V_I, E_I)$ referred to
as the \emph{implication graph}~\cite{silva1997grasp}. The vertices of this
graph are assigned variables or the special node $\contradiction$, which
represents a contradiction, then $V_I \subseteq \Prop \cup \{\contradiction\}$.
The edges in the implication graph are obtained from the antecedent of each
implied variable: if $\alpha(p) = \clause$ then there is a directed edge,
labelled by $\clause$, from each variable in $\clause$, other than $p$, to $p$.
If unit propagation yields an unsatisfiable clause $\clause_i$, then the special
vertex $\contradiction$, for the empty clause, is used to represent it. In this
case, the antecedent of $\contradiction$ is defined by $\alpha(\contradiction) =
\clause_i$.

The edges of $I$ are formally defined below.  

\begin{definition}
    Let $z, z_1, z_2 \in V_I$ be vertices in $I$, in order to derive the
    conditions for existence of edges in $I$, a number of predicates need to be
    defined first.  The predicate $\gamma(z, \clause)$ takes value 1 if, and
    only if, $z$ is a literal in $\clause$, and is defined as follows:
    \begin{equation}
        \gamma(z, \clause) = 
        \left\{
            \begin{array}{ll}
                1 & \text{if $z \in \clause \lor \neg z \in \clause$} \\
                0 & \text{otherwise}
            \end{array}
        \right.
    \end{equation}
    This predicate can now be used for testing the value of a literal in $z$ in
    a given clause. The predicate $\nu_0(z, \clause)$ takes value 1 if, and only
    if, $z$ is a literal in $\clause$ and its value is $\false$:
    \begin{equation}
        \nu_0(z, \clause) = 
        \left\{
            \begin{array}{ll}
                1 & \text{if $\gamma(z, \clause) \land z \in \clause \land \nu(z) = \false$}\\
                1 & \text{if $\gamma(z, \clause) \land \neg z \in \clause \land
                \nu(z) = \true$}\\
                0 & \text{otherwise}
            \end{array}
        \right.
    \end{equation}
    The predicate $\nu_1(z, \clause)$ takes value 1 if, and only if, $z$ is a
    literal in $\clause$ and its value is $\true$:
    \begin{equation}
        \nu_1(z, \clause) = 
        \left\{
            \begin{array}{ll}
                1 & \text{if $\gamma(z, \clause) \land z \in \clause \land
                \nu(z) = \true$}\\
                1 & \text{if $\gamma(z, \clause) \land \neg z \in \clause \land
                \nu(z) = \false$}\\
                0 & \text{otherwise}
            \end{array}
        \right.
    \end{equation}
    As a result, there is an edge from $z_1$ to $z_2$ in $I$ if, and only if,
    the following predicate takes value 1:
    \begin{equation}
        \epsilon(z_1, z_2) = 
        \left\{
            \begin{array}{ll}
                1 & \text{if $z_2 = \contradiction \land \gamma(z_1, \alpha(\contradiction))$}\\
                1 & \text{if $z_2 \neq \contradiction \land \alpha(z_2) = \clause \land \nu_0(z_1, \clause) \land \nu_1(z_2, \clause)$}\\
                0 & \text{otherwise}
            \end{array}
        \right.
    \end{equation}
    Consequently, the set of edges $E_I$ of the implication graph $I$ is given by:
    \begin{equation}
        E_I = \{ (z_1, z_2) \mid \epsilon(z_1, z_2) = 1\}
    \end{equation}
\end{definition}

Observe that a labelling function for associating a clause with each edge can
also be defined. 

\begin{definition}
    Let $\iota : V_I \times V_I \longrightarrow \formula$ be a labeling
    function. Then $\iota(z_1, z_2)$, with $z_1, z_2 \in V_I$ and $(z_1, z_2)
    \in E_I$, is defined by $\iota(z_1, z_2) = \alpha(z_2)$.
\end{definition}

\begin{example}%
    \label{ex:graph1}
    (Implication graph without conflict). Consider the CNF formula:
    \begin{align*}
        \formula & = \clause_1 \land \clause_2 \land \clause_3\\
                 & = (p \lor t \lor \neg q) \land (p \lor \neg r) \land (q \lor r \lor s)
    \end{align*}
    Assume the decision assignment $t = \false @1$ has been taken. Moreover,
    assume that the current decision assignment is $p = \false @2$. Unit
    propagation yields the implied assignments $q = \false @2$, $r = \false @2$
    and $s = \true @2$. The resulting implication graph is shown in
    Figure~\ref{fig:graph1}. As all variables have been assigned a value and the
    implication graph does not contain the vertex $\contradiction$, this set of decision
    assignments forms a satisfying assignment for $\formula$.
    \input{tex/graph1}
\end{example}

\begin{example}%
    \label{ex:graph2}
    (Implication graph with conflict). Consider the CNF formula:
    \begin{align*}
        \psi & = \clause_1 \land \clause_2 \land \clause_3 \land \clause_4 \land \clause_5 \land \clause_6  \\
                 & = (p \lor t \lor \neg q) \land (p \lor \neg r) \land (q \lor r \lor s) \land (\neg s \lor \neg u) \land (y \lor \neg s \lor \neg x) \land (u \lor x)
    \end{align*}
    Assume the decision assignments $y = \false @2$ and $t = \false @3$.
    Moreover, assume the current decision assignment $p = \false @5$.  Unit
    propagation yields the implied assignments $q = \false @2$, $r = \false @2$,
    $s = \true @2$, $u = \false @5$ and $x = false @5$. These last two
    assignments generate a conflict because the clause $\clause_6$ can not be
    satisfied. Therefore, the resulting implication graph has the conflict
    vertex, as shown in Figure~\ref{fig:graph2}, with $\alpha(\contradiction) =
    \clause_6$.  Hence, this set of decision assignments does not satisfy
    $\psi$.
    \input{tex/graph2}
\end{example}

Algorithm~\ref{alg:cdcl}, adapted from~\cite{cdclchapter}, shows the standard
structure of a CDCL SAT solver, which essentially follows the one from DPLL\@.
With respect to DPLL, the main differences are the call to function
\texttt{ConflictAnalysis} each time a conflict is identified, and the call to
\texttt{Backtrack} when backtracking takes place. Moreover, the
\texttt{Backtrack} procedure allows for backtracking non-chronologically. 

\begin{algorithm}[!ht]
    \SetKwFunction{uprop}{UnitPropagate}
    \SetKwFunction{alvarass}{AllVariablesAssigned}
    \SetKwFunction{pickvar}{PickBranchingVariable}
    \SetKwFunction{conflict}{ConflictAnalysis}
    \SetKwFunction{backtrack}{Backtrack}
    \If{\uprop{$\formula,\passignment$} yields a conflict}%
    {\Return~UNSAT}
    $dl \leftarrow$ 0

    \While{$\neg$\alvarass{$\formula$, $\passignment$}}
    {%
        $l \leftarrow$ \pickvar{$\formula$, $\passignment$}\\
        $dl \leftarrow dl + 1$\\
        $\passignment \leftarrow \passignment \cup \{l\}$\\
        \If{\uprop{$\formula,\passignment$} yields a conflict}%
        {%
            $\beta \leftarrow$ \conflict{$formula$, $\passignment$}\\
            \If{$\beta < 0$}
            {\Return~UNSAT}
            \Else{%
                \backtrack{$\formula$,$\passignment$,$\beta$}\\
                $dl \leftarrow \beta$\\
            }
        }
    }
    \caption{CDCL$(\formula, \passignment)$}%
\label{alg:cdcl}
\end{algorithm}

In addition to the main CDCL function, the following auxiliary functions are
used:
\begin{itemize}
    \item \texttt{UnitPropagate}:
        same as in DPLL, consists of the iterated application of the unit
        propagation procedure. If an unsatisfiable clause is identified, then a
        conflict indication is returned.
    \item \texttt{AllVariablesAssigned}:
        tests whether all variables have been assigned, in which case the
        algorithm terminates indicating that the CNF formula is satisfiable. 
    \item \texttt{PickBranckingVariable}:
        consists of selecting a variable to assign and deciding its value.
    \item \texttt{ConflictAnalysis}:
        consists of analysing the most recent conflict and learning a new clause
        from the conflict. The organization of this procedure is described in
        Section~\ref{sec:conflictanalysis}.
    \item \texttt{Backtrack}:
        backtracks to the decision level computed by \texttt{ConflictAnalysis}.
\end{itemize}

Arguments to the auxiliary functions are assumed to be passed by reference.
Hence, $\formula$ and $\passignment$ are supposed to be modified during execution
of these functions.

The typical CDCL algorithm, shown in Algorithm~\ref{alg:cdcl}, does not account
for a few often used techniques, as for instance, search restarts and deletion
policies. Search restarts cause the algorithm to restart itself, but keeping the
learnt clauses.  Clause deletion policies are used to decide learnt clauses that
can be deleted, which allows the memory usage of the SAT solver to be kept under
control. If interested, the reader can refer to~\cite{cdclchapter}
and~\cite{satchapter} for more information about these techniques.

\subsection{Conflict Analysis}%
\label{sec:conflictanalysis}

Each time the CDCL SAT solver identifies a conflict due to unit propagation, the
conflict analysis procedure is invoked. As a result, one or more new clauses are
learnt, and a backtracking decision level is computed. This procedure analyses
the structure of unit propagation and decides which literals to include in the
learnt clause.

The decision levels associated with assigned variables define a partial order
over the variables. Starting from a given unsatisfiable clause (represented in
the implication graph as the vertex $\contradiction$), the conflict analysis
procedure visits variables implied at the most recent decision level. This means
that this procedure will visit the graph vertex which represents the variables
assigned at the current largest decision level. Then, it identifies the
antecedents of these variables and keeps from the antecedents the literals
assigned at decision levels less than the one being considered. This process is
repeated until the most recent decision variable is visited.

Let $d$ be the current decision level and $p$ the current decision variable such that
$\delta(p) = d$. Let $\nu(p) = v$ be the decision assignment and let $\clause$
be an unsatisfiable clause identified with unit propagation. In terms of the
implication graph, the conflict vertex $\contradiction$ is such that
$\alpha(\contradiction) = \clause$. Moreover, take $\res$ to represent the
process of applying the resolution principle, that is, for two clauses
$\clause_i$ and $\clause_j$ containing contradictory literals, $\clause_i \res
\clause_j$ denotes the application of the resolution rule as defined
by~Equation~\ref{eq:res}. For instance, if $l$ and $\neg l$ are literals of
$\clause_i$ and $\clause_j$, respectively, $\clause_i \res \clause_j$ contains
all the literals of both clauses with the exception of the contradictory pair.

The clause learning procedure used in SAT solvers can be defined by a sequence
of selective resolution operations~\cite{journals/corr/abs-1107-0044,DAC00*675},
that at each step yields a \emph{new temporary clause}. 

\begin{definition}%
\label{def:clauselearning}
    Let $l$ be a literal of a clause $\clause$, and $d$ the current decision
    level. Then, consider the following predicate:

    \begin{equation}%
        \label{eq:appliedliteral}
        \xi(\clause, l, d) = 
        \left\{
            \begin{array}{ll}
                1 & \text{if $l \in \clause \land \delta(l) = d \land \alpha(l) \neq nil$} \\
                0 & \text{otherwise}
            \end{array}
        \right.
    \end{equation}

    Then, using the predicate defined by Equation~\ref{eq:appliedliteral}, let
    $\clause^{d,i}$, with $i = 0, 1, \ldots$, be a intermediate clause obtained
    as follows:

    \begin{equation}%
        \label{eq:newclause}
        \clause^{d,i} = 
        \left\{
            \begin{array}{ll}
                \alpha(\contradiction) & \text{if $i=0$}\\
                \clause^{d,i-1} \res \alpha(l) & \text{if $i \neq 0 \land \xi(\clause^{d,i-1}, l, d) = 1$}\\
                \clause^{d, i-1} & \text{if $i \neq 0 \land \forall\;l\;\xi(\clause^{d, i-1}, l, d) = 0$}
            \end{array}
        \right.
    \end{equation}

\end{definition}

The predicate $\xi$ holds if, and only if, the implied literal $l$ of $\clause$
is assigned a value at the current decision level $d$.   

Equation~\ref{eq:newclause} can be used for formalizing the clause learning
procedure. The first condition, $i = 0$, denotes the initialization step given
after $\contradiction$ is generated in $I$, where all literals in the
unsatisfiable
clause are added to the first intermediate clause. Afterwards, at each step $i$,
a literal $l$ assigned at the current decision level $d$ is selected and the
intermediate clause $\clause^{d, i-1}$ is resolved with the antecedent of $l$.
Therefore, $\clause^{d,i}$ denotes the clause obtained after $i$ resolution
operations.

An iteration $i$ such that $\clause^{d, i} = \clause^{d, i-1}$ represents a
reached \emph{fixed point}, and $\clause_L \stackrel{def} \clause^{d, i}$
denotes the new learnt clause. Observe that the number of resolution
operations represented by Equation~\ref{eq:newclause} is no greater than
$|\Prop_\formula|$.

\begin{example}%
\label{ex:cl}
    (Clause learning)
    Consider Example~\ref{ex:graph2}. The application of clause learning to this
    example results in the intermediate clauses shown in Table~\ref{tab:cl}. The
    resulting learnt clause is $\clause_L = \clause^{5,6} = (p \lor t \lor y)$.
    Alternatively, this clause can be obtained by inspecting the graph in
    Figure~\ref{fig:graph2} and selecting the negation of the literals assigned
    at decision levels less than the current decision level 5, i.e. $t = \false
    @3$ and $y = \false @ 2$, and by selecting the current decision assignment
    $p = \false @5$.

\begin{table}%
\centering
\caption{Resolution steps during clause learning}
    \begin{tabular}{ll}
        \toprule
        $\clause^{5,0} = \{ u, x\}$ & Literals in $\alpha(\contradiction)$ \\
        $\clause^{5,1} = \{ \neg s, x\}$ & Resolve with $\alpha(u) = \clause_4$ \\
        $\clause^{5,2} = \{ \neg s, y\}$ & Resolve with $\alpha(x) = \clause_5$ \\
        $\clause^{5,3} = \{ q, r, y\}$ & Resolve with $\alpha(s) = \clause_3$ \\
        $\clause^{5,4} = \{ p, t, r, y\}$ & Resolve with $\alpha(q) = \clause_1$ \\
        $\clause^{5,5} = \{ p, t, y\}$ & Resolve with $\alpha(r) = \clause_2$ \\
        $\clause^{5,6} = \clause_L = \{ p, t, y\}$ & No more resolution operations given \\
        \toprule
    \end{tabular}%   
\label{tab:cl}
\end{table}
\end{example}

Modern SAT solvers implement an additional refinement of
Definition~\ref{def:clauselearning}, by further exploiting the structure of
implied assignments induced by unit propagation, which is a key aspect of the
clause learning procedure~\cite{silva1997grasp}. This aspect was further
explored with \emph{Unit Implication Points} (UIPs)~\cite{silva1997grasp}. A UIP
is a vertex $u$ in the implication graph, such that every path from a decision
vertex $p$ to the conflict vertex $\contradiction$ contains $u$. It represents
an alternative decision assignment at the current decision level that results in
the same conflict. The main motivation for identifying UIPs is to reduce the
size of learnt clauses. 

In the implication graph, there is a UIP at decision level $d$ when the number
of literals in $\clause_L$ assigned at decision level $d$ is
1~\cite{cdclchapter}. 

\begin{definition}
    Let $\phi(\clause,d)$ be the number of literals in $\clause$ assigned at
    decision level $d$:
    \begin{equation}
        \phi(\clause,d) = |\{l \in \clause\;|\;\delta(l) = d\}|
    \end{equation}
    As a result, the clause learning procedure with UIPs is given by:
    \begin{equation}%
        \label{eq:newclause_uip}
        \clause^{d,i} = 
        \left\{
            \begin{array}{ll}
                \alpha(\contradiction) & \text{if $i=0$}\\
                \clause^{d,i-1} \res \alpha(l) & \text{if $i \neq 0 \land \xi(\clause^{d,i-1}, l, d) = 1$}\\
                \clause^{d, i-1} & \text{if $i \neq 0 \land \phi(\clause^{d,i-1}, d) = 1$}
            \end{array}
        \right.
    \end{equation}
\end{definition}

Equation~\ref{eq:newclause_uip} allows creating a clause containing literals
from the learnt clause until the first UIP is identified. It is simple to
develop equations for learning clauses for each additional UIP\@. However, this
is unnecessary in practice, the most effective CDCL SAT solvers stop
clause learning when the first one is reached. Moreover, clause learning could
potentially stop at any UIP\@. But, considering the largest decision level of the
literals of the clause learnt at each UIP, the clause learnt at the first UIP is
guaranteed to contain the smallest one. What follows from this is that choosing
the clause learnt at the first UIP implies the highest backtrack jump in the
search tree~\cite{cdclchapter}. 

\begin{example}
    Consider again Example~\ref{ex:graph2}. The default clause learning
    procedure would learn the clause $(p \lor t \lor y)$ (see
    Example~\ref{ex:cl}). However, by taking into consideration that $s =
    \true @5$ is a UIP, applying the clause learning of
    Equation~\ref{eq:newclause_uip} yields the resulting clause $(\neg s \lor
    y)$. One advantage of this new clause is that it has one less literal than
    the learnt clause from Example~\ref{ex:graph2}.
\end{example}

Clause learning finds other applications besides the key efficiency improvements
to CDCL SAT solvers. One example is \emph{clause reuse}. In a large number of
applications, clauses learnt from a given CNF formula can often be reused for
related CNF formulae. The basic concept is to reuse constraints on the search
space which are deduced while checking a previous instance, for speeding up the
SAT checking of the current one~\cite{shtrichman2001pruning}.

Moreover, for unsatisfiable subformulae, the clauses learnt by a CDCL SAT
solver, as formalised in Definition~\ref{def:clauselearning}, encode a
resolution refutation of the original formula. Given the way clauses are learnt
in this solvers, each learnt clause can be explained by a number of resolution
steps, each of which is a trivial resolution step. As a result, the resolution
refutation can be obtained from the learnt clauses in linear time and space on
the number of learnt clauses. 

For unsatisfiable formulae, the resolution refutations obtained from the clauses
learnt by a SAT solver serve as certificate for validating the correctness of
the SAT solver~\cite{cdclchapter}. Besides, resolution refutations based on
clause learning find practical applications, including hardware model
checking~\cite{mcmillan2003interpolation}.

\section{Modern SAT Solvers}%
\label{sec:modern}

Annual competitions have led to the development of dozens of clever
implementations of SAT solvers, allowing many techniques to be explored and
creating an extensive suite of real-world instances as well as challenging
hand-crafted benchmarks problems~\cite{satchapter}. Apart from conflict
analysis, modern solvers include techniques like lazy data structures, search
restarts, conflict-driven branching heuristics and clause deletion
strategies~\cite{cdclchapter}.  

In 2001, researches at Princeton University designed a complete solver, which
they named Chaff~\cite{moskewicz2001chaff}, that, as most solvers, is an
instance of the DPLL search procedure with a number of enhancements aiming in
efficient applications.  Their algorithm was later used as base for numerous
well-succeed implementations (as~\cite{chrabakh2003gridsat} and~\cite{minisat},
for instance). Chaff achieved significant performance gains through careful
engineering of all aspects of the search --- especially a particularly efficient
implementation of unit propagation and a novel low overhead decision strategy.
It was able to obtain, at the time, one to two orders of magnitude performance
improvement on difficult SAT benchmarks~\cite{moskewicz2001chaff}.

Alongside the optimized unit propagation, Chaff also designed a decision
heuristic, which they called \emph{Variable State Independent Decaying Sum
(VSIDS)}. Decision assignment consists of the determination of which new
variable should be assigned a value, in each decision level. There are several
strategies in the literature to try and solve this problem, but by that far,
there was a lack of clear statistical evidence supporting one strategy over
others, making difficult to determine what differs a good decision from a bad
one~\cite{moskewicz2001chaff}. With that in mind, and also after some testing
with a few available strategies, Chaff's designers thought they could come up
with a considerably better heuristic for the range of problems on which Chaff
was being tested. The VSIDS strategy is described as follows:

\begin{enumerate}
    \item Each literal has a counter, initialized to 0.
    \item When a clause is added to the clause database, the counter associated with
        each literal in the clause is incremented.
    \item The unassigned literal with the highest counter is chosen at each
        decision.
    \item Ties are broken randomly by default.
    \item Periodically, all the counters are divided by a constant.
\end{enumerate}

Overall, this strategy can be viewed as attempting to satisfy the conflict
clauses but, particularly, attempting to satisfy \emph{recent} conflict clauses,
due to the fifth item. Since difficult problems generate many conflicts, the
conflict clauses dominate the problem in terms of literal count, so this
approach distinguishes itself in how it favors the information generated by
recent conflict clauses~\cite{moskewicz2001chaff}. To this day, CDCL SAT solvers
crucially depend on the VSIDS decision heuristic for their
performance~\cite{vsidsincdcl}. 

A few years later, S\"orensson and Een developed a SAT solver with conflict
clause minimization, named \emph{MiniSat}~\cite{minisat}, that stayed for a
while as the state-of-the-art solver. MiniSat is a popular high-performance
satisfiability solver that implements many well-known performance heuristics in
a concise manner. It has won several prizes in the SAT 2005 competition, and has
the advantage of being open-source. 

MiniSat is the implementation of the solver described
in~\cite{een2003extensible}. It corresponds to a minimalistic Chaff-like SAT
solver based on clause learning by conflict analysis. A number of small
improvements has been made with respect to the original Chaff. Two important
features are the incremental SAT interface, and the support for user defined
boolean constrains. Although none of these improvements are really relevant for
the SAT competition, they are important when using MiniSat as an integrated part
of a bigger system~\cite{minisat}. In this sense, a user application can specify
and solve SAT-problems, through MiniSat's external interface, sending its own
previously known boolean constraints.

MiniSat's version of VSIDS makes some simple changes to the original one, as
follows:

\begin{enumerate}
    \item Each \emph{variable} has a counter.
    \item Instead of a decay factor, variable counters are ``bumped'' with
        larger and larger values in a floating point representation. When very
        large values are encountered, all counters are scaled down.
    \item At every certain rate, a random decision is made instead.
        This factor is set at run-time.
\end{enumerate}
In addition, MiniSat's implementation of VSIDS always keeps the variables in
order by placing them in an array-based priority queue.

In 2009, a MiniSat based solver appeared claiming to predict the quality of
learnt clauses. \emph{Glucose}~\cite{glucose} is based on a scoring scheme
introduced earlier on the same year by Audemard and Simon
in~\cite{audemard2009predicting}. The name of the solver is a contraction of the
concept of ``glue clauses'', a particular kind of clauses that Glucose detects
and preserves during the search.

Detecting what is a good learnt clause in advance is a challenge, and of first
importance: deleting useful clauses can increase time dramatically, in
practice~\cite{glucose}. To prevent this, solvers have to let the maximum
number of clauses grow exponentially. On very hard benchmarks, CDCL solvers face
memory problems and, even if they do not, their greedy learning scheme
deteriorates the performance.  In~\cite{audemard2009predicting}, Audemard and
Simon showed that a very simple static measure on clauses can dramatically
improves the performance of the publicly available version of MiniSat of that
time.

In their studies, Audemard and Simon observed a strong relationship between the
overall decreasing of decision levels, presented by most solvers running on a
industrial set of benchmarks, and the performance of
them~\cite{audemard2009predicting}.  In this sense, they thought that finding
the part of the learning schema that enforces this decreasing rate, could lead
to identifying good learnt clauses in advance.

During the search, each decision is often followed by a large number of unit
propagations. All literals from the same level are called ``blocks'' of literals
in the later. Intuitively, at the semantic level, there is a chance that they
are linked with each other by direct dependencies. The idea is that a good
learning schema should add explicit links between independent blocks of decision
literals. If the solver stays in the same search space, such a clause will
probably help reducing the number of next decision levels in the remaining
computation~\cite{audemard2009predicting}.

In Glucose, literals in a clause are partitioned with respect to their decision
level. The \emph{Literals Blocks Distance (LBD)} of a clause is exactly the
number of subsets of its literals, divided accordingly to the current
assignment. This measure is static, even if updated during the search (the LBD
score of a clause can be re-computed when the clause is used in
unit-propagation). For instance, it is easy to understand the importance of
learnt clauses of LBD 2: they only contain one variable of a previous decision
level, and, later, this variable will be ``glued'' with the block of literals
propagated above, no matter the size of the clause. These clauses are very
important during the search, and they receive a special name: ``glue clauses''.

Experiment results lead Audemard and Simon to conclude that this strattegy
works in practice. In most of the cases, the decreasing rate of decision
levels was indeed accelarated.

One can notice that these last decades showed an enormous progress in the
performance of SAT solvers. We have seen these modern solvers increasingly
leaving their mark as a general-purpose tool, which provides a ``black-box''
procedure that can often solve hard structured problems with over a million
variables and several million constrains. 

We believe that we can take advantage of the theoretical and practical efforts
that have been directed in improving the efficiency of such solvers. As
mentioned in Chapter~\ref{3_Resolution}, the \ksp\ prover loses in performance
when there is a high number of propositional symbols in just one particular
level. We already know that these sets are satisfiable, as the specific normal
form used always generates satisfiable sets of propositional clauses. If we
feed a SAT solver, like MiniSat or Glucose, with these satisfiable sets, each
time a conflict is identified, we think that the learnt clauses may help \ksp\
to reduce the time spend during saturation.
