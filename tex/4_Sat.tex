%\section{}
The problem of determining whether a formula in classical propositional logic is
satisfiable has the historical honor of being the first problem ever shown to be
NP-Complete~\cite{Cook}. Great theoretical and practical efforts have been
directed in improving the efficiency of solvers for this problem, known as
\emph{Boolean Satisfiability Solvers}, or just \emph{SAT solvers}. Despite the
worst-case deterministic exponential run time of all the algorithms known,
satisfiability solvers are increasingly leaving their mark as a general purpose
tool in the most diverse areas~\cite{satchapter}. In essence, SAT solvers
provide a generic combinatorial reasoning and search platform. Beyond that, the
source code of many implementations of such solvers is freely available and can
be used as a basis for the development of decision procedures for more
expressive logics~\cite{giunchiglia2002sat}.

In the context of SAT solvers for propositional provers, the underlying
representational formalism is propositional logic~\cite{satchapter}. We are
interested in formulae in \emph{Conjunctive Normal Form} (CNF): $\formula$ is in
CNF if it is a conjunction of clauses. For example, $\formula = (p \lor \neg q)
\land (\neg p \lor r \lor s) \land (q \lor r)$ is a CNF formula with four
variables and three clauses. A clause with only one literal is referred to as a
\emph{unit clause}, and a clause with two literals, as a \emph{binary clause}.

%% satisfying assignment %%
A propositional formula $\formula$ takes a value in the set $\{\false, \true\}$. In
algorithms for SAT, variables can be \emph{assigned} a logic value in the same
set and, alternatively, variables may also be \emph{unassigned}. A \emph{truth
assignment} (or just an assignment) to a set of variables $\Prop$, is the valuation
function $\pi$ as defined in Definition~\ref{def:semantics}. As in propositional
logic we have a unit set as the set of possible worlds $\St$, we can omit this
set from the function signature and just write $\pi : \Prop \longrightarrow
\{\false,\true\}$, for simplicity. A \emph{satisfying assignment} for $\formula$
is an assignment $\pi$ such that $\formula$ evaluates to $\true$ under $\pi$.  A
\emph{partial assignment} for a formula $\formula$ is a truth assignment to a
subset of the variables in $\formula$. For a partial assignment $\passignment$
for a CNF formula $\formula$, $\formula|_\passignment$ denotes the simplified
formula obtained by replacing the variables appearing in $\passignment$ with
their specified values, removing all clauses with at least one $\true$ literal,
and deleting all occurrences of $\false$ literals from the remaining
clauses~\cite{satchapter}.   

Therefore, the \emph{Boolean Satisfiability Problem} (SAT) can be expressed as:
Given a CNF formula $\formula$, does $\formula$ have a satisfying assignment? If
this is the case, $\formula$ is said to be \emph{satisfiable}, otherwise,
$\formula$ is \emph{unsatisfiable}.  One can be interested not only in the
answer of this decision problem, but also in finding the actual assignment that
satisfies the formula, when it exists. All practical SAT solvers do produce such
an assignment~\cite{cormen}. 

\section{The DPLL Procedure}%
\label{sec:dpll}

A \emph{complete} solution method for the SAT problem is one that, given the
input formula $\formula$, either produces a satisfying assignment for $\formula$
or proves that it is unsatisfiable~\cite{satchapter}. One of the most surprising
aspects of the relatively recent practical progress of SAT solvers is that the
best complete methods remain variants of a process introduced in the early
1960â€™s: the Davis-Putnam-Logemann-Loveland, or DPLL,
procedure~\cite{DavisLongemannLoveland:1962}, which describes a backtracking
algorithm to the search problem of finding a satisfying assignment for a formula in
the space of partial assignments. A key feature of DPLL is efficient pruning of
the search space based on falsified clauses. Since its introduction, the main
improvements to DPLL have been smart branch selection heuristics, extensions
like clause learning and randomized restarts, and well-crafted data structures
such as lazy implementations and watched literals for fast unit
propagation~\cite{satchapter}.

Algorithm~\ref{alg:dpll}, DPLL-recursive$(\formula, \passignment)$, where
$\passignment$ corresponds to a partial assignment of the CNF formula
$\formula$, sketches the basic DPLL procedure on CNF
formulae~\cite{DavisLongemannLoveland:1962}. The main idea is to repeatedly
select an unassigned literal $l$ in the input formula and recursively search for
a satisfying assignment for $\formula|_l$ and $\formula|_{\neg l}$. The step
where such an $l$ is chosen is called a \emph{branching step}. Setting $l$ to
$\true$ or $\false$ when making a recursive call is referred to as
\emph{decision}, and is associated with a \emph{decision level} which equals the
recursion depth at that stage of the procedure. The end of each recursive call,
which takes $\formula$ back to fewer assigned literals, is called the
\emph{backtracking step}.

\begin{algorithm}[htp]%
    \SetAlgoLined\DontPrintSemicolon%
    \SetKwFunction{proc}{UnitPropagate}
    $(\formula, \passignment) \leftarrow$ \proc{$\formula,\passignment$}

    \If{$\formula$ contains the empty clause}
    {\Return~UNSAT}
    \If{$\formula$ has no clauses left}
    {Output $\passignment$\\
    \Return{SAT}
    }
    $l \leftarrow$ a literal not assigned by $\passignment$

    \If{DPLL-recursive$(\formula|_l, \passignment \cup \{l\}) = $ SAT} 
    {\Return{SAT}}
    \Return{DPLL-recursive$(\formula|_{\neg l}, \passignment \cup \{\neg l\})$}

    \vspace{2mm}
    \setcounter{AlgoLine}{0}
    \SetKwProg{myproc}{sub}{}{}
    \myproc{\proc{$\formula, \passignment$}}{%
    \While{$\formula$ contains no empty clause but has a unit clause $\clause$}
    {%
        $l \leftarrow$ the literal in $\clause$ not assigned by $\passignment$\\
        $\formula \leftarrow \formula|_l$\\
        $\passignment \leftarrow \passignment \cup \{\l\}$
    }
    \Return{$(\formula, \passignment)$}
    }
    \caption{DPLL-recursive$(\formula, \passignment$)}%
    \label{alg:dpll}
\end{algorithm} 

A partial assignment $\passignment$ is maintained during the search and output if the
formula turns out to be satisfiable.  To increase efficiency, a key procedure in SAT
solvers is the \emph{unit propagation}~\cite{cdclchapter}, where unit clauses
are immediately set to $\true$ as outlined in Algorithm~\ref{alg:dpll}. In most
implementations of DPLL, logical inferences can be derived with unit propagation.
Thus, this procedure is used for identifying variables which must be assigned
a specific value. If $\formula |_\passignment$ contains the empty clause, a
\emph{conflict} condition is declared, the corresponding clause of $\formula$
from which it came is said to be \emph{violated} by $\passignment$, and the algorithm
backtracks. The literals whose negation do not appear in the formula, called
\emph{pure literals}, are also set to $\true$ as a preprocessing step and, in
some implementations, during the simplification process after every branching. 

Variants of this algorithm form the most widely used family of complete
algorithms for the SAT problem. They are frequently implemented in an iterative
manner, resulting in significantly reduced memory usage. The efficiency of
state-of-the-art SAT solvers relies heavily on various features that have been
developed, analysed and tested over the last two decades. These include fast
unit propagation using watched literals, deterministic and randomized restart
strategies, effective clause deletion mechanisms, smart static and dynamic
branching heuristics and learning mechanisms. We will discuss learning
mechanisms in the next section and refer the reader to~\cite{satchapter} for
more details about other strategies.

\section{Conflict-Driven Clause Learning}%
\label{sec:cdcl}

One of the main reasons for the widespread use of SAT in many applications is
that solvers based on clause learning are effective in
practice~\cite{satchapter}. The main idea is to cache ``causes of conflict'' as
learned clauses, and utilize this information to prune the search in a different
part of the search space encountered later. Since their inception in the
mid-90s, \emph{Conflict-Driven Clause Learning} (CDCL) SAT solvers have been
applied, in many cases with remarkable success, to a number of practical
applications~\cite{cdclchapter}. The organization of CDCL SAT solvers is
primarily inspired by the DPLL procedure.

In CDCL SAT solvers, each variable $p$ is characterized by a number of
properties, including the \emph{value}, the \emph{antecedent} and the
decision level, denoted respectively by $\nu(p) \in \{\false, \true, u\}$, where
$\nu(p) = u$ means that $p$ is still unassigned, $\alpha(p) \in \formula \cup
\{nil\}$, and $\delta(p) \in \{-1, 0, 1, \ldots, |\Prop|\}$. A variable $p$ that
is assigned a value as the result of unit propagation is said to be
\emph{implied}.  The unit clause $\clause$ used for implying this value is said
to be the antecedent of $p$, that is, $\alpha(p) = \clause$. For variables that
are decision variables or are unassigned, the antecedent is $nil$. Hence,
antecedents are only defined for variables whose value is implied by other
assignments. The decision level of a variable $p$ denotes the depth of the
decision tree at which the variables is assigned a value in $\{\false, \true\}$
or $\delta(p) = -1$ if $p$ is still unassigned. The decision level associated
with variables used for branching steps is specified by the search process, and
denotes the current depth of the \emph{decision stack}. Hence, a variable $p$
associated with a decision assignment is characterized by having $\alpha(p) =
nil$ and $\delta(p) > 0$. More formally, the decision level of $p$ with
antecedent $\clause$ is given by: 
\begin{equation}
    \delta(p) = \max(\{0\} \cup \{\delta(p') \mid p' \in \clause \land p' \neq p\})
\end{equation}
i.e.\ the decision of an implied literal is either the highest decision level of
the implied literals in a unit clause, or it is 0 in case the clause is unit.
The notation $p = v @ d$ is used to denote that $\nu(p) = v$ and $\delta(p) =
d$. Moreover, the decision level of a literal is defined as the decision level
of its variable, that is, $\delta(l) = \delta(p)$ if $l = p$ or $l = \neg p$.

\begin{example}
    Consider the formula 
    \begin{align*}
        \formula & = \clause_1 \land \clause_2 \land \clause_3\\
                 & = (p \lor \neg s) \land (p \lor r) \land (\neg r \lor q \lor s)
    \end{align*}
    Assume that the decision assignment is $s = \false @ 1$. Unit propagation
    yields no additional implied assignments. Assume that the second decision is
    $p = \false @ 2$. Unit propagation yields the implied assignments $r = \true
    @ 2$ and $q = \true @ 2$. Therefore, $\pi = \{(s, \false), (p, \false), (r,
    \true), (q, \true)\}$ is a satisfying assignment for $\formula$, since $\pi$
    makes $\formula$ true. Moreover, $\alpha(r) = \clause_2$ and $\alpha(q) =
    \clause_3$.
\end{example}

During the execution of a DPLL based SAT solver, assigned variables as well as
their antecedents define a directed acyclic graph $I = (V_I, E_I)$ referred to as
the \emph{implication graph}~\cite{silva1997grasp}. The vertices of this graph
are defined by all assigned variables and one special node $\contradiction$, $V_I
\subseteq \Prop \cup \{\contradiction\}$. The edges in the implication graph are
obtained from the antecedent of each assigned variable: if $\alpha(p) = \clause$
then there is a directed edge from each variable in $\clause$, other than $p$, to
$p$. If unit propagation yields an unsatisfied clause $\clause_i$, then a special
vertex $\contradiction$ is used to represent the unsatisfied clause. In this case, the
antecedent of $\contradiction$ is defined by $\alpha(\contradiction) = \clause_i$.

The edges of $I$ are formally defined below. Let $z, z_1, z_2 \in V_I$ be
vertices in $I$, in order to derive the conditions for existence of edges in
$I$, a number of predicates need to be defined first. 

\begin{definition}
    The predicate $\gamma(z, \clause)$ takes value 1 if, and only if, $z$
    is a literal in $\clause$, and is defined as follows:
    \begin{equation}
        \gamma(z, \clause) = 
        \left\{
            \begin{array}{ll}
                1 & \text{if $z \in \clause \lor \neg z \in \clause$} \\
                0 & \text{otherwise}
            \end{array}
        \right.
    \end{equation}
    This predicate can now be used for testing the value of a literal in $z$ in
    a given clause. The predicate $\nu_0(z, \clause)$ takes value 1 if, and only
    if, $z$ is a literal in $\clause$ and its value is $\false$:
    \begin{equation}
        \nu_0(z, \clause) = 
        \left\{
            \begin{array}{ll}
                1 & \text{if $\gamma(z, \clause) \land z \in \clause \land \nu(z) = \false$}\\
                1 & \text{if $\gamma(z, \clause) \land \neg z \in \clause \land
                \nu(z) = \true$}\\
                0 & \text{otherwise}
            \end{array}
        \right.
    \end{equation}
    The predicate $\nu_1(z, \clause)$ takes value 1 if, and only if, $z$ is a
    literal in $\clause$ and its value is $\true$:
    \begin{equation}
        \nu_1(z, \clause) = 
        \left\{
            \begin{array}{ll}
                1 & \text{if $\gamma(z, \clause) \land z \in \clause \land
                \nu(z) = \true$}\\
                1 & \text{if $\gamma(z, \clause) \land \neg z \in \clause \land
                \nu(z) = \false$}\\
                0 & \text{otherwise}
            \end{array}
        \right.
    \end{equation}
    As a result, there is an edge from $z_1$ to $z_2$ in $I$ if, and only if,
    the following predicate takes value 1:
    \begin{equation}
        \epsilon(z_1, z_2) = 
        \left\{
            \begin{array}{ll}
                1 & \text{if $z_2 = \contradiction \land \gamma(z_1, \alpha(\contradiction))$}\\
                1 & \text{if $z_2 \neq \contradiction \land \alpha(z_2) = \clause \land \nu_0(z_1, \clause) \land \nu_1(z_2, \clause)$}\\
                0 & \text{otherwise}
            \end{array}
        \right.
    \end{equation}
\end{definition}

Consequently, the set of edges $E_I$ of the implication graph $I$ is given by:
\begin{equation}
    E_I = \{ (z_1, z_2) \mid \epsilon(z_1, z_2) = 1\}
\end{equation}

Finally, observe that a labeling function for associating a clause with each
edge can also be defined. 

\begin{definition}
    Let $\iota : V_I \times V_I \longrightarrow \formula$ be a labeling
    function. Then $\iota(z_1, z_2)$, with $z_1, z_2 \in V_I$ and $(z_1, z_2)
    \in E_I$, is defined by $\iota(z_1, z_2) = \alpha(z_2)$.
\end{definition}

\begin{example}%
    \label{ex:graph1}
    (Implication graph without conflict). Consider the CNF formula:
    \begin{align*}
        \formula & = \clause_1 \land \clause_2 \land \clause_3\\
                 & = (p \lor t \lor \neg q) \land (p \lor \neg r) \land (q \lor r \lor s)
    \end{align*}
    Assume the decision assignment $t = \false @1$ has been taken. Moreover,
    assume that the current decision assignment is $p = \false @2$. Unit
    propagation yields the implied assignments $q = \false @2$, $r = \false @2$
    and $s = \true @2$. The resulting implication graph is shown in
    Figure~\ref{fig:graph1}. As all variables have been assigned a value and the
    implication graph does not contain the vertex $\contradiction$, this set of decision
    assignments forms a satisfying assignment for $\formula$.
    \input{tex/graph1}
\end{example}

\begin{example}%
    \label{ex:graph2}
    (Implication graph with conflict). Consider the CNF formula:
    \begin{align*}
        \psi & = \clause_1 \land \clause_2 \land \clause_3 \land \clause_4 \land \clause_5 \land \clause_6  \\
                 & = (p \lor t \lor \neg q) \land (p \lor \neg r) \land (q \lor r \lor s) \land (\neg s \lor \neg u) \land (y \lor \neg s \lor \neg x) \land (u \lor x)
    \end{align*}
    Assume the decision assignments $y = \false @2$ and $t = \false @3$.
    Moreover, assume the current decision assignment $p = \false @5$.  Unit
    propagation yields the implied assignments $q = \false @2$, $r = \false @2$,
    $s = \true @2$, $u = \false @5$ and $x = false @5$. These last two
    assignments generate a conflict once the clause $\clause_6$ becomes
    unsatisfied. Therefore, the resulting implication graph has the conflict
    vertex, as shown in Figure~\ref{fig:graph2}, with $\alpha(\contradiction) =
    \clause_6$.  Hence, this set of decision assignments does not satisfy
    $\psi$.
    \input{tex/graph2}
\end{example}

Algorithm~\ref{alg:cdcl}, adapted from~\cite{cdclchapter}, shows the standard
structure of a CDCL SAT solver, which essentially follows the one from DPLL\@.
With respect to DPLL, the main differences are the call to function
\texttt{ConflictAnalysis} each time a conflict is identified, and the call to
\texttt{Backtrack} when backtracking takes place. Moreover, the
\texttt{Backtrack} procedure allows for backtracking non-chronologically. 

\begin{algorithm}[!ht]
    \SetKwFunction{uprop}{UnitPropagate}
    \SetKwFunction{alvarass}{AllVariablesAssigned}
    \SetKwFunction{pickvar}{PickBranchingVariable}
    \SetKwFunction{conflict}{ConflictAnalysis}
    \SetKwFunction{backtrack}{Backtrack}
    \If{\uprop{$\formula,\passignment$} yields a conflict}%
    {\Return~UNSAT}
    $dl \leftarrow$ 0

    \While{$\neg$\alvarass{$\formula$, $\passignment$}}
    {%
        $l \leftarrow$ \pickvar{$\formula$, $\passignment$}\\
        $dl \leftarrow dl + 1$\\
        $\passignment \leftarrow \passignment \cup \{l\}$\\
        \If{\uprop{$\formula,\passignment$} yields a conflict}%
        {%
            $\beta \leftarrow$ \conflict{$formula$, $\passignment$}\\
            \If{$\beta < 0$}
            {\Return~UNSAT}
            \Else{%
                \backtrack{$\formula$,$\passignment$,$\beta$}\\
                $dl \leftarrow \beta$\\
            }
        }
    }
    \caption{CDCL$(\formula, \passignment)$}%
\label{alg:cdcl}
\end{algorithm}

In addition to the main CDCL function, the following auxiliary functions are
used:
\begin{itemize}
    \item \texttt{UnitPropagate}:
        same as in DPLL, consists of the iterated application of the unit
        propagation procedure. If an unsatisfied clause is identified, then a
        conflict indication is returned.
    \item \texttt{AllVariablesAssigned}:
        tests whether all variables have been assigned, in which case the
        algorithm terminates indicating that the CNF formula is satisfiable. 
    \item \texttt{PickBranckingVariable}:
        consists of selecting a variable to assign and deciding its value.
    \item \texttt{ConflictAnalysis}:
        consists of analyzing the most recent conflict and learning a new clause
        from the conflict. The organization of this procedure is described in
        Section~\ref{sec:conflictanalysis}.
    \item \texttt{Backtrack}:
        backtracks to the decision level computed by \texttt{ConflictAnalysis}.
\end{itemize}

Arguments to the auxiliary functions are assumed to be passed by reference.
Hence, $\formula$ and $\passignment$ are supposed to be modified during execution
of these functions.

The typical CDCL algorithm shown does not account for a few often used
techniques, as for instance, search restarts and deletion policies. Search
restarts cause the algorithm to restart itself, but keeping the learnt clauses.
Clause deletion policies are used to decide learnt clauses that can be deleted,
which allows the memory usage of the SAT solver to be kept under control.

\subsection{Conflict Analysis}%
\label{sec:conflictanalysis}

Each time the CDCL SAT solver identifies a conflict due to unit propagation, the
conflict analysis procedure is invoked. As a result, one or more new clauses are
learnt, and a backtracking decision level is computed. This procedure analyses
the structure of unit propagation and decides which literals to include in the
learnt clause.

The decision levels associated with assigned variables define a partial order of
the variables. Starting from a given unsatisfied clause (represented in the
implication graph as the vertex $\contradiction$), the conflict analysis
procedure visits variables implied at the most recent decision level, i.e., the
current largest decision level, identifies the antecedents of these variables
and keeps from the antecedents the literals assigned at decision levels less
than the one being considered. This process is repeated until the most recent
decision variable is visited.

Let $d$ be the current decision level, let $p$ be the decision variable, let
$\nu(p) = v$ be the decision assignment and let $\clause$ be an unsatisfied
clause identified with unit propagation. In terms of the implication graph, the
conflict vertex $\contradiction$ is such that $\alpha(\contradiction) = \clause$. Moreover, take
$\res$ to be the resolution operator. For two clauses $\clause_i$ and
$\clause_j$, for which there is a unique variable $q$ such that one clause has a
literal $q$ and the other has literal $\neg q$, $\clause_i \res \clause_j$
contains all the literals of $\clause_i$ and $\clause_j$ with the exception of
$q$ and $\neg q$.

The clause learning procedure used in SAT solvers can be defined by a sequence
of selective resolution operations~\cite{journals/corr/abs-1107-0044,DAC00*675},
that at each step yields a new temporary clause. 

\begin{definition}%
\label{def:clauselearning}
    First, define a predicate that holds if a clause $\clause$ has an implied
    literal $l$ assigned at the current decision level $d$:

    \begin{equation}%
        \label{eq:appliedliteral}
        \xi(\clause, l, d) = 
        \left\{
            \begin{array}{ll}
                1 & \text{if $l \in \clause \land \delta(l) = d \land \alpha(l) \neq nil$} \\
                0 & \text{otherwise}
            \end{array}
        \right.
    \end{equation}

    Let $\clause^{d, i}$, with $i = 0, 1, \ldots$, be the intermediate clause
    obtained after $i$ resolution operations. Using the predicated defined
    by Equation~\ref{eq:appliedliteral}, this intermediate clause can be defined
    as follows:

    \begin{equation}%
        \label{eq:newclause}
        \clause^{d,i} = 
        \left\{
            \begin{array}{ll}
                \alpha(\contradiction) & \text{if $i=0$}\\
                \clause^{d,i-1} \res \alpha(l) & \text{if $i \neq 0 \land \xi(\clause^{d,i-1}, l, d) = 1$}\\
                \clause^{d, i-1} & \text{if $i \neq 0 \land \forall\;l\;\xi(\clause^{d, i-1}, l, d) = 0$}
            \end{array}
        \right.
    \end{equation}
\end{definition}

Equation~\ref{eq:newclause} can be used for formalizing the clause learning
procedure. The first condition, $i = 0$, denotes the initialization step given
in $\contradiction$ in $I$, where all literals in the unsatisfied clause are added to
the first intermediate clause. Afterwards, at each step $i$, a literal $l$
assigned at the current decision level $d$ is selected and the intermediate
clause $\clause^{d, i-1}$ is resolved with the antecedent of $l$.

For an iteration $i$ such that $\clause^{d, i} = \clause^{d, i-1}$, then a
\emph{fixed point} is reached, and $\clause_L \stackrel{def} \clause^{d, i}$
represents the new learnt clause. Observe that the number of resolution
operations represented by Equation~\ref{eq:newclause} is no greater than
$|\Prop|$.

\begin{example}%
\label{ex:cl}
    (Clause learning)
    Consider Example~\ref{ex:graph2}. The application of clause learning to this
    example results in the intermediate clauses shown in Table~\ref{tab:cl}. The
    resulting learnt clause is $\clause_L = \clause^{5,6} = (p \lor t \lor y)$.
    Alternatively, this clause can be obtained by inspecting the graph in
    Figure~\ref{fig:graph2} and selecting the literals assigned at decision
    levels less than the current decision level 5, i.e. $t = \false @3$ and $y =
    \false @ 2$, and by selecting the current decision assignment $p = \false
    @5$.
\end{example}

\begin{table}%
\centering
\caption{Resolution steps during clause learning}
    \begin{tabular}{ll}
        \toprule
        $\clause^{5,0} = \{ u, x\}$ & Literals in $\alpha(\contradiction)$ \\
        $\clause^{5,1} = \{ \neg s, x\}$ & Resolve with $\alpha(u) = \clause_4$ \\
        $\clause^{5,2} = \{ \neg s, y\}$ & Resolve with $\alpha(x) = \clause_5$ \\
        $\clause^{5,3} = \{ q, r, y\}$ & Resolve with $\alpha(s) = \clause_3$ \\
        $\clause^{5,4} = \{ p, t, r, y\}$ & Resolve with $\alpha(q) = \clause_1$ \\
        $\clause^{5,5} = \{ p, t, y\}$ & Resolve with $\alpha(r) = \clause_2$ \\
        $\clause^{5,6} = \{ p, t, y\}$ & No more resolution operations given \\
        \toprule
    \end{tabular}%   
\label{tab:cl}
\end{table}

Modern SAT solvers implement an additional refinement of
Definition~\ref{def:clauselearning}, by further exploiting the structure of
implied assignments induced by unit propagation, which is a key aspect of the
clause learning procedure~\cite{silva1997grasp}. The idea of exploiting the
structure induced by unit propagation was further exploited with \emph{Unit
Implication Points} (UIPs). A UIP is a vertex $u$ in the implication graph, such
that every path from the decision vertex $p$ to the conflict vertex $\contradiction$
contains $u$, and it represents an alternative decision assignment at the
current decision level that results in the same conflict. The main motivation
for identifying UIPs is to reduce the size of learnt clauses. 

Clause learning finds other applications besides the key efficiency improvements
to CDCL SAT solvers. One example is \emph{clause reuse}. In a large number of
applications, clauses learnt from a given CNF formula can often be reused for
related CNF formulae. 

Moreover, for unsatisfiable subformulae, the clauses learnt by a CDCL SAT solver
encode a resolution refutation of the original formula. Given the way clauses
are learnt in this solvers, each learnt clause can be explained by a number of
resolution steps, each of which is a trivial resolution step. As a result, the
resolution refutation can be obtained from the learnt clauses in linear time and
space on the number of learnt clauses. 

For unsatisfiable formulae, the resolution refutations obtained from the clauses
learnt by a SAT solver serve as certificate for validating the correctness of
the SAT solver. Moreover, resolution refutations based on clause learning find
practical applications~\cite{cdclchapter}.

Besides allowing the production of a resolution refutation, learnt clauses also
allow identifying a subset of clauses that is also unsatisfiable. For example, a
\emph{minimally unsatisfiable subformula} can be derived by iteratively removing
a single clause and checking unsatisfiability. Unnecessary clauses are
discarded, and eventually a minimally unsatisfiable subformula is obtained.

\section{Modern CDCL Solvers}%

Apart from conflict analysis, modern solvers include techniques like lazy data
structures, search restarts, conflict-driven branching heuristics and clause
deletion strategies~\cite{cdclchapter}.  

\subsection{MiniSat}%
\label{sec:minisat}

\subsection{Glucose}
