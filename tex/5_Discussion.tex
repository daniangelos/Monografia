%\section{}
%\section{Related Work}
In~\cite{giunchiglia2002sat}, Giunchiglia et.\ al.\ present a set of SAT-based
decision procedures for various classical modal logics. These procedures are
built on top of a SAT solver. This differ from our work in the sense that we are
not developing a system from an existing solver. Our implementation instantiates
a SAT solver to feed it with sets we already know are satisfiable.

Chapter~\ref{2_Language} introduced the propositional modal language which is
the main focus of this work: \system{K}{n}{}. We have seen that modal logics
extend classical logic by adding operators to express one or more of different
modes of truth.  The key goal behind these operators is to allow the reasoning
over relations among the abstractions we call possible worlds. Thus, the
information holding at worlds accessible from the current one --- via an
accessibility relation --- is available to examination.  

\system{K}{n}{} is the extension of the classical propositional logic plus the
unary operators $\nec{a}$ and $\pos{a}$, whose reading are ``is necessary from
the point of view of an agent $a$'' and ``is possible from the point of view of
an agent $a$'', respectively. \system{K}{n}{}' syntax follow directly from the
syntax of classical propositional language, also considering constructions with
the modal operators as well-formed formulae.

On the other hand, the semantics of \system{K}{n}{} is presented in terms of
Kripke models. A set of worlds, their binary accessibility relations, labelled
by an agent, and a valuation function define a structure known as a Kripke
model. The satisfiability and validity of a formula depend on this
structure.

Finally, Chapter~\ref{2_Language} also presented what proof systems and normal
forms are. Recall that we are interested in sound and complete calculus for
\system{K}{n}{}, as well as normal form translations that preserve
satisfiability of formulae.

In Chapter~\ref{3_Resolution}, we mentioned \ksp, the theorem prover presented
in~\cite{Nalon2016}. Earlier in the same chapter, we briefly introduced clausal
resolution, the base of the modal-layered calculus behind \ksp. As we have seen,
resolution is a simple and adaptable proof system for propositional classical
logic, as it has only one inference rule that is thoroughly applied. This rule
entails the well known resolution principle.

The calculus presented for \system{K}{n}{} adds a few more rules, as it also has
to handle with modal reasoning. This calculus makes use of labelled resolution
in order to avoid unnecessary application of such rules. Thus, it requires a
translation of formulae into a more expressive language, where labels are used
to express semantic properties. Formulae in \system{K}{n}{} is, then, translated
into a layered normal form called \snf{$ml$}. We showed that a formula in
\snf{$ml$} is a conjunction of clauses where the modal level in which they occur
is made explicit in the syntax. We referred the reader to where one can find the
proofs of correctness of such calculus and translation.

\ksp~implements this modal labelled resolution calculus. It was designed to
support experimentation with different combinations of refinements.
\ksp~performs well if the set of propositional symbols are uniformly distributed
over the modal levels. However, when there is a high number of propositional
symbols in just one particular level, the performance deteriorates. One reason
is that the specific normal form used always generates satisfiable sets of
propositional clauses (clauses without modal operators). As resolution relies on
saturation, this can be very time consuming. 

We saw in Chapter~\ref{4_Sat} that SAT solvers can often solve hard structured
problems with over a million variables and several million
constraints~\cite{satchapter}. We believe that we can take advantage of the
theoretical and practical efforts that have been directed in improving the
efficiency of such solvers. 

Also in Chapter~\ref{4_Sat}, we discussed the role of clause learning in the
successful widespread use of SAT\. We saw that the main idea behind
Conflict-Driven Clause Learning solvers is to cache ``clauses of conflict'' as
learned clauses, and to use this information to prune the search in a different
part of the search space. The conflict analysis procedure consists of analyzing
the most recent conflict and learning a new clause from it.

Our implementation, which is work in progress, uses a SAT solver based on clause
learning by conflict analysis. We feed this solver with the satisfiable sets of
clauses generated. Each time a conflict is identified in these sets due to unit
propagation from some variable assignment, one or more new clauses are learnt
from the conflict analysis procedure, that analyses the structure of unit
propagation and decides which literals to include in the new
clauses~\cite{cdclchapter}. As we already know that these sets are satisfiable,
we are not particularly interested in the satisfying assignment generated by the
SAT solver, but we believe that by carefully choosing the set of clauses and
making use of the learnt clauses generated by the solver, we may be able to
reduce the time \ksp~spends during saturation. 

%example of workflow

\section{MiniSat vs Glucose}
In the Section~\ref{sec:modern}, we gave an overview of two of the most
efficient SAT solvers known to this date: MiniSat and Glucose. Both solvers are
great candidates to be combined with \ksp. This section discuss these two
implementations. 

As we have seen, MiniSat is a minimalistic solver, that implements a variation
of Chaff's Variable State Independent Decaying Sum. MiniSat stayed for a long as
the state-of-the-art solver. To these days, this solver keeps its
importance as a tool used as an integrated part of a different system. In this
sense, MiniSat's public interface and support for user defined constrains are
key features.

It is through MiniSat's interface, taken from~\cite{een2003extensible} and
illustrated below, that a user application can specify and solve SAT
problems~\cite{een2003extensible}. 

\begin{lstlisting}[frame=single, language=C++,morekeywords={var,Solver,Vec,lit},escapechar=\%,
commentstyle=\color{gray}]
class Solver _ Public interface
%\indent%var %\textit{newVar}%()
%\indent%bool %\textit{addClause}%(Vec%$\langle$%lit%$\rangle$% literals)
%\indent%bool %\textit{add}\ldots(\ldots)%
%\indent%bool %\textit{solve}%(Vec%$\langle$%lit%$\rangle$% assumptions)

%\indent%Vec%$\langle$%bool%$\rangle$% model //If found, this vector has the model
\end{lstlisting}

The ``\textit{add}\ldots'' method should be understood as a place-holder for
additional constraints implemented in an extension of MiniSat. For a standard
SAT problem, the interface is used in the following way: Variables are
introduced by calling ``\textit{newVar}''. From these variables, clauses are
built and added by ``\textit{addClause}''.  Trivial conflicts, such as two unit
clauses $\{p\}$ and $\{\neg p\}$ being added, can be detected by this method, in
which case it returns $\cfalse$. If no such trivial conflict is detected during
the clause insertion phase, ``\textit{solve}'' is called with an empty list of
assumptions. It returns $\cfalse$ if the set of clauses added is unsatisfiable,
and $\ctrue$ if it is satisfiable, in which case the model can be read from the
public vector ``model''.  If the solver returns satisfiable, new constrains can
be added repeatedly to the existing database and ``\textit{solve}'' may execute again. 

The search procedure of a modern solver is usually the most complex part to
implement~\cite{een2003extensible}. Heuristically, variables are picked and
assigned values until the propagation detects a conflict. At this point, a
conflict clause is construct and added to the clauses database. Variables are
then unassigned by backtracking until the conflict clause becomes unit, from
which point this unit clause is propagated and the search process continues.

The learning procedure of MiniSat starts when a constraint becomes impossible to
satisfy under the current assignment. The conflicting constraint is then asked
for a set of variable assignments that make it contradictory. For a clause, this
would be all the literals of the conflict clause. Each of the variable
assignments returned must be either a decision or a implied variable. The
propagation constraints are in turn asked for the set of variable assignments
that forced the propagation to occur, continuing the analysis backwards. The
procedure is repeated until some termination condition is fulfilled, resulting
in a set of variable assignments that implies the conflict. A clause prohibiting
that particular assignment is added to the clause database. This learnt clause
must always, by construction, be implied by the original problem constraints.

Learnt clauses serve two purposes: they drive the backtracking (as showed in
Section~\ref{sec:cdcl}) and they speed up future conflicts by caching the reason
for the conflict. 

The main mechanism in which MiniSat runs, taken from~\cite{een2003extensible},
is illustrated bellow.

\begin{lstlisting}[frame=single, language=C++,morekeywords={var,loop,then,lit},escapechar=\%,
commentstyle=\color{gray},deletekeywords={not}]
loop
%\indent%%\textit{propagate()}% /*propagate unit clauses*/
%\indent%if not conflict then
%\indent%%\indent%if all variables assigned then
%\indent%%\indent%%\indent%return %\texttt{satisfiable}%
%\indent%%\indent%else
%\indent%%\indent%%\indent%%\textit{decide()}% /*pick a new variable to assign a value*/ 
%\indent%else
%\indent%%\indent\textit{analyze()}% /*analyze conflict and add a conflict clause*/
%\indent\indent%if top-level conflict found then
%\indent\indent\indent%return %\texttt{unsatisfiable}%
%\indent\indent%else
%\indent\indent\indent\textit{backtrack()}% /*undo assignments*/
\end{lstlisting}

To use this prover in our work, we would have to interact with the public
interface of an instance of MiniSat, feed it with the clauses at a specific
modal level, and capture the learnt clause (or clauses) returned by the
procedure \textit{analyze\.()}, when a conflict is identified. Finally, we would
add this clause to the set of propositional clauses of \ksp, at the
corresponding modal level, hoping this new clause reduces the time \ksp~spends
dealing with propositional logic. We believe that only a few changes in the
source code of MiniSat would be necessary.

On the other hand, we have Glucose, which dramatically improved the performance
of the solver that it was based on: MiniSat~\cite{glucose}, claiming to predict
the quality of learn clauses. As we are interested in the clauses a solver will
learn from the set we feed it, this may highlight Glucose from other SAT
solvers. However, we still have to investigate if the metrics they use to judge
this so called quality, really apply to our problem. 

Most CDCL solvers present a decreasing of decision level on most industrial
benchmarks~\cite{audemard2009predicting}. In their studies, Audemard and Simon
observed a strong relationship between this overall decreasing and the
performance of the solver~\cite{audemard2009predicting}. They refer as
\emph{look-back justification} the estimated number of conflicts before reaching
the first decision level, which concludes the search. One curious thing they
noticed was that the look-back justification is also strong when the solver
finds a solution, i.e., a satisfying assignment. This suggested that, on
satisfiable instances of problems, the solver does not correctly guess a value
for a literal, but learns that the opposite value directly leads to a
contradiction. In this sense, they thought that finding the part of the learning
schema that enforces the decreasing rate of decision levels, would lead to
identifying good learnt clauses in advance.

As we saw in Section~\ref{sec:modern}, during the search, each decision is often
followed by a large number of unit propagations. All literals from the same
level are called ``blocks'' of literals in the later, meaning that there is a
chance that they are linked with each other by direct dependencies. The idea is
that a good learning schema should add explicit links between independent blocks
of decision literals. If the solver stays in the same search space, such a
clause will probably help reducing the decision levels in the remaining
computation.

We also saw that Glucose partitions the literals in a clause with respect to
their decision level. The LBD of a clause is exactly the number of subsets of
its literals, divided accordingly to the current assignment. The LBD score of
each learnt clause is computed and stored when it is produced. This measure is
static, even if updated during the search (LBD score of a clause can be
re-computed when the clause is used in unit-propagation). 

For instance, it is easy to understand the importance of learnt clauses of LBD
2: they only contain one variable of a previous decision level, and, later, this
variable will be ``glued'' with the block of literals propagated above, no
matter the size of the clause. These clauses are very important during the
search, and they receive the special name ``glue clauses''.

Experiment results lead Audemard and Simon to conclude that this strategy
works in practice. In most of the cases, the decreasing rate of decision
levels was indeed accelarated.

The question we face is to either go with the minimalism of MiniSat, or to
bet on Glucose's robust search for a good learnt clause.

We believe that the amount of work to combine \ksp~with either provers would be
quite close. Since Glucose is based on MiniSat, they share a similar user
interface. Other than that, both provers are written in C++, with a comparable
amount of documentation. MiniSat has an experimental version in C, a language we
are more familiar with, although, it is a distant version from the last public
release of MiniSat, in 2010. Glucose's last release, on the other hand, is from
2016. 

Since it has a newer release, and as all the effort it makes to produce the
so called good clauses seems to be worth it, Glucose is the chosen prover.

\section{Future Work}%
\label{sec:future}

Our choice of going with Glucose is decisive, but flexible. Our studies, the
overview of the solvers' code and modest tests lead us to it, so we are
confident. However, if in practice we suspect that MiniSat (or any other
solver), would actually work better for our purpose, we intend to revisit this
decision. In the meantime, we will be moderately changing Glucose to best fit
our necessities.

Then, we will focus on integrating Glucose with \ksp. During the main
loop of \ksp, numerous system calls may be made to execute an instance of our
own slightly different version of Glucose, passing as parameters, propositional
clauses at the specific modal level. This integration will be implemented as a
choice to the user, as most strategies of \ksp, to maintain the experimentation
support.

When the implementation is working well, is time to define our hypothesis, the
metrics for testing, and choose the benchmarks.

Finally, we will put \ksp~through a large amount of test. Then all the results
will be collected and critically analysed, aiming to evaluate the impact of
combining Glucose with \ksp.

Summing up:
\begin{itemize}
    \item Fit Glucose to our necessities while evaluating the choice of the solver,
    \item Automated integration with \ksp,
    \item Define metrics for tests and select benchmarks,
    \item Exhaustively testing,
    \item Collect and analyse data.
\end{itemize}

%%comment on evaluating the tests and the choices
%automated integration
%set option on ksp
%selecting benchmarks
%exhaustively testing
